{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\"\"\"\n",
    "    Linear regression using the Ridge method\n",
    "\"\"\"\n",
    "\n",
    "def RidgeRegression(x, y, z, degree=5, l=0.0001):\n",
    "    \"\"\"\n",
    "    :param x: numpy vector of size (n, 1)\n",
    "    :param y: numpy vector of size (n, 1)\n",
    "    :param degree: degree of polynomial fit\n",
    "    :param l: Ridge penalty coefficient\n",
    "    :return: numpy array with the beta coefficients\n",
    "    \"\"\"\n",
    "    # Calculate matrix with x, y - polynomials\n",
    "    M_ = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    M = poly.fit_transform(M_)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate beta\n",
    "    A = np.arange(1, degree + 2)\n",
    "    rows = np.sum(A)\n",
    "    beta = (np.linalg.inv(M.T.dot(M) + l * np.identity(rows))).dot(M.T).dot(z)\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to use in analysis of a regression method\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "def FrankeFunction(x,y, noise=0.01):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return (term1 + term2 + term3 + term4 + noise*np.random.randn(len(x)))\n",
    "\n",
    "def R2(zReal, zPredicted):\n",
    "    \"\"\"\n",
    "    :param zReal: actual z-values, size (n, 1)\n",
    "    :param zPredicted: predicted z-values, size (n, 1)\n",
    "    :return: R2-score\n",
    "    \"\"\"\n",
    "    meanValue = np.mean(zReal)\n",
    "    numerator = np.sum((zReal - zPredicted)**2)\n",
    "    denominator = np.sum((zReal - meanValue)**2)\n",
    "    result = 1 - (numerator/denominator)\n",
    "    return result\n",
    "\n",
    "def MeanSquaredError(z, z_hat):\n",
    "    \"\"\"\n",
    "    :param z: actual z-values, size (n, 1)\n",
    "    :param z_hat: predicted z-values, size (n, 1)\n",
    "    :return: Mean squared error\n",
    "    \"\"\"\n",
    "    MSE = np.sum((z - z_hat)**2)/len(z)\n",
    "    return MSE\n",
    "\n",
    "def betaConfidenceInterval_Ridge(z_real, beta, X, l):\n",
    "    \"\"\"\n",
    "    Compute a 90% confidence interval for the beta coefficients - Ridge\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate variance squared in the error\n",
    "    z_hat = X.dot(beta)\n",
    "    N, P = np.shape(X)\n",
    "    sigma_2 = (np.sum(np.power((z_real-z_hat), 2)))/N\n",
    "\n",
    "    # Calculate the variance squared of the beta coefficients\n",
    "    XTX= X.T.dot(X)\n",
    "    R, R = np.shape(XTX)\n",
    "    var_beta = np.diag(sigma_2*np.linalg.inv((XTX + l*np.identity(R))))\n",
    "\n",
    "    # The square root of var_beta is the standard error. Use it to calculate confidence intervals\n",
    "    i_minus = beta - 1.645*np.sqrt(var_beta/N)\n",
    "    i_plus = beta + 1.645*np.sqrt(var_beta/N)\n",
    "\n",
    "    return i_minus, i_plus\n",
    "\n",
    "def betaConfidenceInterval_OLS(z_real, beta, X):\n",
    "    \"\"\"\n",
    "    Compute a 90% confidence interval for the beta coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate variance squared in the error\n",
    "    z_hat = X.dot(beta)\n",
    "    N, P = np.shape(X)\n",
    "    sigma_2 = (np.sum(np.power((z_real-z_hat), 2)))/N\n",
    "\n",
    "    # Calculate the variance squared of the beta coefficients\n",
    "    var_beta = np.diag(sigma_2*np.linalg.inv((X.T.dot(X))))\n",
    "\n",
    "    # The square root of var_beta is the standard error. Use it to calculate confidence intervals\n",
    "    i_minus = beta - 1.645*np.sqrt(var_beta/N)\n",
    "    i_plus = beta + 1.645*np.sqrt(var_beta/N)\n",
    "\n",
    "    return i_minus, i_plus\n",
    "\n",
    "def plotFrankes(beta, degree=5):\n",
    "    \"\"\"\n",
    "    Plot Franke's function\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.arange(0, 1, 0.01)\n",
    "    y = np.arange(0, 1, 0.01)\n",
    "\n",
    "    x_, y_ = np.meshgrid(x, y)\n",
    "    x = x_.reshape(-1,1)\n",
    "    y = y_.reshape(-1,1)\n",
    "\n",
    "    M = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    M_ = poly.fit_transform(M)\n",
    "    predict = M_.dot(beta)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(x_, y_, predict.reshape(100, 100), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Ordinary Least Squared function\n",
    "def ols(x, y, z, degree = 5):\n",
    "    #x: vector of size(n, 1)\n",
    "    #y: vector of size(n,1)\n",
    "    # z: vector of size(n,1)\n",
    "    xyb_ = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    xyb = poly.fit_transform(xyb_)\n",
    "    beta = np.linalg.inv(xyb.T.dot(xyb)).dot(xyb.T).dot(z)\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "def bootstrap(x, y, z, p_degree, method, n_bootstrap=100):\n",
    "    # Randomly shuffle data\n",
    "    data_set = np.c_[x, y, z]\n",
    "    np.random.shuffle(data_set)\n",
    "    set_size = round(len(x)/5)\n",
    "\n",
    "    # Extract test-set, never used in training. About 1/5 of total data\n",
    "    x_test = data_set[0:set_size, 0]\n",
    "    y_test = data_set[0:set_size, 1]\n",
    "    z_test = data_set[0:set_size, 2]\n",
    "    test_indices = np.linspace(0, set_size-1, set_size)\n",
    "\n",
    "    # And define the training set as the rest of the data\n",
    "    x_train = np.delete(data_set[:, 0], test_indices)\n",
    "    y_train = np.delete(data_set[:, 1], test_indices)\n",
    "    z_train = np.delete(data_set[:, 2], test_indices)\n",
    "\n",
    "    Z_predict = []\n",
    "\n",
    "    MSE = []\n",
    "    R2s = []\n",
    "    for i in range(n_bootstrap):\n",
    "        x_, y_, z_ = resample(x_train, y_train, z_train)\n",
    "\n",
    "        if method == 'Ridge':\n",
    "            # Ridge regression, save beta values\n",
    "            beta = RidgeRegression(x_, y_, z_, degree=p_degree)\n",
    "        elif method == 'Lasso':\n",
    "            beta = Lasso(x_, y_, z_, degree=p_degree)\n",
    "        elif method == 'OLS':\n",
    "            beta = ols(x_, y_, z_, degree=p_degree)\n",
    "        else:\n",
    "            print('ERROR: Cannot recognize method')\n",
    "            return 0\n",
    "\n",
    "        M_ = np.c_[x_test, y_test]\n",
    "        poly = PolynomialFeatures(p_degree)\n",
    "        M = poly.fit_transform(M_)\n",
    "        z_hat = M.dot(beta)\n",
    "\n",
    "        Z_predict.append(z_hat)\n",
    "\n",
    "        # Calculate MSE\n",
    "        MSE.append(np.mean((z_test - z_hat)**2))\n",
    "        R2s.append(R2(z_test, z_hat))\n",
    "        print('Round: ', i)\n",
    "\n",
    "    # Calculate MSE, Bias and Variance\n",
    "    MSE_M = np.mean(MSE)\n",
    "    R2_M = np.mean(R2s)\n",
    "    bias = np.mean((z_test - np.mean(Z_predict, axis=0, keepdims=True))**2)\n",
    "    variance = np.mean(np.var(Z_predict, axis=0, keepdims=True))\n",
    "    return MSE_M, R2_M, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model with bootstrap\n",
    "X = np.load('data.npy')\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = FrankeFunction(x, y, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE, R2_b, bias, variance = bootstrap(x, y, z, method='OLS', p_degree=5)\n",
    "print('--- BOOTSTRAP for OLS ---')\n",
    "print('MSE: ', MSE)\n",
    "print('R2: ', R2_b)\n",
    "print('Bias: ', bias)\n",
    "print('Variance: ', variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "x_test = np.random.rand(1000)\n",
    "y_test = np.random.rand(1000)\n",
    "z_test = FrankeFunction(x_test, y_test, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate beta values and polynomial matrix\n",
    "beta = RidgeRegression(x, y, z, degree=5, l=10**-4)\n",
    "M_ = np.c_[x_test, y_test]\n",
    "poly5 = PolynomialFeatures(5)\n",
    "M = poly5.fit_transform(M_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate beta confidence intervals\n",
    "conf1, conf2 = betaConfidenceInterval_OLS(z_test, beta, M)\n",
    "\n",
    "for i in range(len(conf1)):\n",
    "    print('Beta {0}: {1:5f} & [{2:5f}, {3:5f}]'.format(i, beta[i], conf1[i], conf2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to use in analysis of a regression method\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def FrankeFunction(x,y, noise = 0):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return (term1 + term2 + term3 + term4 + noise*np.random.randn(len(x)))\n",
    "\n",
    "\n",
    "def R2(zReal, zPredicted):\n",
    "    \"\"\"\n",
    "    :param zReal: actual z-values, size (n, 1)\n",
    "    :param zPredicted: predicted z-values, size (n, 1)\n",
    "    :return: R2-score\n",
    "    \"\"\"\n",
    "    meanValue = np.mean(zReal)\n",
    "    numerator = np.sum((zReal - zPredicted)**2)\n",
    "    denominator = np.sum((zReal - meanValue)**2)\n",
    "    result = 1 - (numerator/denominator)\n",
    "    return result\n",
    "\n",
    "def MeanSquaredError(z, z_hat):\n",
    "    \"\"\"\n",
    "    :param z: actual z-values, size (n, 1)\n",
    "    :param z_hat: predicted z-values, size (n, 1)\n",
    "    :return: Mean squared error\n",
    "    \"\"\"\n",
    "    MSE = np.sum((z - z_hat)**2)/len(z)\n",
    "    return MSE\n",
    "\n",
    "def betaCI_OLS(z_real, beta, X):\n",
    "    \"\"\"\n",
    "    Compute a 90% confidence interval for the beta coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate variance squared in the error\n",
    "    z_hat = X.dot(beta)\n",
    "    N, P = np.shape(X)\n",
    "    sigma2 = (np.sum(np.power((z_real-z_hat), 2)))/N\n",
    "\n",
    "    # Calculate the variance squared of the beta coefficients\n",
    "    var_beta = np.diag(sigma2*np.linalg.inv((X.T.dot(X))))\n",
    "\n",
    "    # The square root of var_beta is the standard error. Use it to calculate confidence intervals\n",
    "    ci_minus = beta - 1.645*np.sqrt(var_beta/N)\n",
    "    ci_plus = beta + 1.645*np.sqrt(var_beta/N)\n",
    "\n",
    "    return ci_minus, ci_plus\n",
    "\n",
    "\n",
    "def betaCI_Ridge(z_real, beta, X, l):\n",
    "    \"\"\"\n",
    "    Compute a 90% confidence interval for the beta coefficients - Ridge\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate variance squared in the error\n",
    "    z_hat = X.dot(beta)\n",
    "    N, P = np.shape(X)\n",
    "    sigma_2 = (np.sum(np.power((z_real-z_hat), 2)))/N\n",
    "\n",
    "    # Calculate the variance squared of the beta coefficients\n",
    "    XTX= X.T.dot(X)\n",
    "    R, R = np.shape(XTX)\n",
    "    var_beta = np.diag(sigma_2*np.linalg.inv((XTX + l*np.identity(R))))\n",
    "\n",
    "    # The square root of var_beta is the standard error. Use it to calculate confidence intervals\n",
    "    ci_minus = beta - 1.645*np.sqrt(var_beta/N)\n",
    "    ci_plus = beta + 1.645*np.sqrt(var_beta/N)\n",
    "\n",
    "    return ci_minus, ci_plus\n",
    "\n",
    "def plotFrankes(x_, y_, z_):\n",
    "    \"\"\"\n",
    "    Plot Franke's function\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    surf = ax.plot_surface(x_, y_, z_, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(-0.10, 1.40)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z - Franke')\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    clb = fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    clb.ax.set_title('Level')\n",
    "\n",
    "    plt.savefig('./Figure/Franke.pdf')\n",
    "\n",
    "#Ordinary Least Squared function\n",
    "def ols(x, y, z, degree = 5):\n",
    "    #x: vector of size(n, 1)\n",
    "    #y: vector of size(n,1)\n",
    "    #z: vector of size(n,1)\n",
    "    xyb_ = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    xyb = poly.fit_transform(xyb_)\n",
    "    beta = np.linalg.inv(xyb.T.dot(xyb)).dot(xyb.T).dot(z)\n",
    "\n",
    "    return beta\n",
    "\n",
    "def RidgeRegression(x, y, z, degree=5, l=0.0001):\n",
    "    \"\"\"\n",
    "    :param x: numpy vector of size (n, 1)\n",
    "    :param y: numpy vector of size (n, 1)\n",
    "    :param degree: degree of polynomial fit\n",
    "    :param l: Ridge penalty coefficient\n",
    "    :return: numpy array with the beta coefficients\n",
    "    \"\"\"\n",
    "    # Calculate matrix with x, y - polynomials\n",
    "    M_ = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    M = poly.fit_transform(M_)\n",
    "\n",
    "    # Calculate beta\n",
    "    A = np.arange(1, degree + 2)\n",
    "    rows = np.sum(A)\n",
    "    beta = (np.linalg.inv(M.T.dot(M) + l * np.identity(rows))).dot(M.T).dot(z)\n",
    "\n",
    "    return beta\n",
    "\n",
    "def bootstrap(x, y, z, p_degree, method, n_bootstrap=100):\n",
    "    # Randomly shuffle data\n",
    "    data_set = np.c_[x, y, z]\n",
    "    np.random.shuffle(data_set)\n",
    "    set_size = round(len(x)/5)\n",
    "\n",
    "    # Extract test-set, never used in training. About 1/5 of total data\n",
    "    x_test = data_set[0:set_size, 0]\n",
    "    y_test = data_set[0:set_size, 1]\n",
    "    z_test = data_set[0:set_size, 2]\n",
    "    test_indices = np.linspace(0, set_size-1, set_size)\n",
    "\n",
    "    # And define the training set as the rest of the data\n",
    "    x_train = np.delete(data_set[:, 0], test_indices)\n",
    "    y_train = np.delete(data_set[:, 1], test_indices)\n",
    "    z_train = np.delete(data_set[:, 2], test_indices)\n",
    "\n",
    "    Z_predict = []\n",
    "\n",
    "    MSE = []\n",
    "    R2s = []\n",
    "    for i in range(n_bootstrap):\n",
    "        x_, y_, z_ = resample(x_train, y_train, z_train)\n",
    "\n",
    "        if method == 'Ridge':\n",
    "            # Ridge regression, save beta values\n",
    "            beta = RidgeRegression(x_, y_, z_, degree=p_degree)\n",
    "        elif method == 'Lasso':\n",
    "            beta = Lasso(x_, y_, z_, degree=p_degree)\n",
    "        elif method == 'OLS':\n",
    "            beta = ols(x_, y_, z_, degree=p_degree)\n",
    "        else:\n",
    "            print('ERROR: Cannot recognize method')\n",
    "            return 0\n",
    "\n",
    "        M_ = np.c_[x_test, y_test]\n",
    "        poly = PolynomialFeatures(p_degree)\n",
    "        M = poly.fit_transform(M_)\n",
    "        z_hat = M.dot(beta)\n",
    "\n",
    "        Z_predict.append(z_hat)\n",
    "\n",
    "        # Calculate MSE\n",
    "        MSE.append(np.mean((z_test - z_hat)**2))\n",
    "        R2s.append(R2(z_test, z_hat))\n",
    "        print('Round: ', i)\n",
    "\n",
    "    # Calculate MSE, Bias and Variance\n",
    "    MSE_M = np.mean(MSE)\n",
    "    R2_M = np.mean(R2s)\n",
    "    bias = np.mean((z_test - np.mean(Z_predict, axis=0, keepdims=True))**2)\n",
    "    variance = np.mean(np.var(Z_predict, axis=0, keepdims=True))\n",
    "    return MSE_M, R2_M, bias, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "Round:  1\n",
      "Round:  2\n",
      "Round:  3\n",
      "Round:  4\n",
      "Round:  5\n",
      "Round:  6\n",
      "Round:  7\n",
      "Round:  8\n",
      "Round:  9\n",
      "Round:  10\n",
      "Round:  11\n",
      "Round:  12\n",
      "Round:  13\n",
      "Round:  14\n",
      "Round:  15\n",
      "Round:  16\n",
      "Round:  17\n",
      "Round:  18\n",
      "Round:  19\n",
      "Round:  20\n",
      "Round:  21\n",
      "Round:  22\n",
      "Round:  23\n",
      "Round:  24\n",
      "Round:  25\n",
      "Round:  26\n",
      "Round:  27\n",
      "Round:  28\n",
      "Round:  29\n",
      "Round:  30\n",
      "Round:  31\n",
      "Round:  32\n",
      "Round:  33\n",
      "Round:  34\n",
      "Round:  35\n",
      "Round:  36\n",
      "Round:  37\n",
      "Round:  38\n",
      "Round:  39\n",
      "Round:  40\n",
      "Round:  41\n",
      "Round:  42\n",
      "Round:  43\n",
      "Round:  44\n",
      "Round:  45\n",
      "Round:  46\n",
      "Round:  47\n",
      "Round:  48\n",
      "Round:  49\n",
      "Round:  50\n",
      "Round:  51\n",
      "Round:  52\n",
      "Round:  53\n",
      "Round:  54\n",
      "Round:  55\n",
      "Round:  56\n",
      "Round:  57\n",
      "Round:  58\n",
      "Round:  59\n",
      "Round:  60\n",
      "Round:  61\n",
      "Round:  62\n",
      "Round:  63\n",
      "Round:  64\n",
      "Round:  65\n",
      "Round:  66\n",
      "Round:  67\n",
      "Round:  68\n",
      "Round:  69\n",
      "Round:  70\n",
      "Round:  71\n",
      "Round:  72\n",
      "Round:  73\n",
      "Round:  74\n",
      "Round:  75\n",
      "Round:  76\n",
      "Round:  77\n",
      "Round:  78\n",
      "Round:  79\n",
      "Round:  80\n",
      "Round:  81\n",
      "Round:  82\n",
      "Round:  83\n",
      "Round:  84\n",
      "Round:  85\n",
      "Round:  86\n",
      "Round:  87\n",
      "Round:  88\n",
      "Round:  89\n",
      "Round:  90\n",
      "Round:  91\n",
      "Round:  92\n",
      "Round:  93\n",
      "Round:  94\n",
      "Round:  95\n",
      "Round:  96\n",
      "Round:  97\n",
      "Round:  98\n",
      "Round:  99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bod\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:153: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "C:\\Users\\bod\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:154: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "C:\\Users\\bod\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:155: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Analysis of bootstrap resampling technique\n",
    "\"\"\"\n",
    "# load data\n",
    "X = np.load('data.npy')\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "MSE, R2_b, bias, variance = bootstrap(x, y, z, method='OLS', p_degree=5)\n",
    "text_file = open(\"./Results/Bootstrap_ols.txt\", \"w\")\n",
    "text_file.write('--- BOOTSTRAP for OLS --- \\n')\n",
    "text_file.write('MSE: {} \\n'.format(MSE))\n",
    "text_file.write('R2: {} \\n'.format(R2_b))\n",
    "text_file.write('Bias: {} \\n'.format(bias))\n",
    "text_file.write('Variance: {} \\n'.format(variance))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to use in analysis of regression methods\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def FrankeFunction(x,y, noise = 0):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return (term1 + term2 + term3 + term4 + noise*np.random.randn(len(x)))\n",
    "\n",
    "\n",
    "def R2(zReal, zPredicted):\n",
    "    \"\"\"\n",
    "    :param zReal: actual z-values, size (n, 1)\n",
    "    :param zPredicted: predicted z-values, size (n, 1)\n",
    "    :return: R2-score\n",
    "    \"\"\"\n",
    "    R2 = 1 - (np.sum((zReal - zPredicted)**2)/np.sum((zReal - np.mean(zReal))**2))\n",
    "    return R2\n",
    "\n",
    "def MeanSquaredError(zReal, zPredicted):\n",
    "    \"\"\"\n",
    "    :param zReal: actual z-values, size (n, 1)\n",
    "    :param zPredicted: predicted z-values, size (n, 1)\n",
    "    :return: Mean squared error\n",
    "    \"\"\"\n",
    "    MSE = np.sum((zReal - zPredicted)**2)/len(z)\n",
    "    return MSE\n",
    "\n",
    "def betaCI_OLS(zReal, beta_mean, X):\n",
    "    \"\"\"\n",
    "    :param zReal: actual z-values, size (n, 1)\n",
    "    :param beta_mean: mean of beta\n",
    "    :param X: dataset\n",
    "    Compute a 90% confidence interval for the beta coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate variance squared in the error\n",
    "    z_hat = X.dot(beta)\n",
    "    N, P = np.shape(X)\n",
    "    sigma2 = (np.sum(np.power((zReal-z_hat), 2)))/N\n",
    "\n",
    "    # Calculate the variance squared of the beta coefficients\n",
    "    var_beta = np.diag(sigma2*np.linalg.inv((X.T.dot(X))))\n",
    "\n",
    "    # The square root of var_beta is the standard error. Confidence intervals are calculated as mean +/- Z*SE\n",
    "    ci_minus = beta_mean - 1.645*var_beta\n",
    "    ci_plus = beta_mean + 1.645*var_beta\n",
    "\n",
    "    return ci_minus, ci_plus\n",
    "\n",
    "\n",
    "def betaCI_Ridge(zReal, beta_mean, X, l):\n",
    "    \"\"\"\n",
    "    :param zReal: actual z-values, size (n, 1)\n",
    "    :param beta_mean: mean of beta\n",
    "    :param X: dataset\n",
    "    Compute a 90% confidence interval for the beta coefficients - Ridge\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate variance squared in the error\n",
    "    z_hat = X.dot(beta)\n",
    "    N, P = np.shape(X)\n",
    "    sigma_2 = (np.sum(np.power((zReal-z_hat), 2)))/N\n",
    "\n",
    "    # Calculate the variance squared of the beta coefficients\n",
    "    XTX= X.T.dot(X)\n",
    "    R, R = np.shape(XTX)\n",
    "    var_beta = np.diag(sigma_2*np.linalg.inv((XTX + l*np.identity(R))))\n",
    "\n",
    "    # The square root of var_beta is the standard error. Confidence intervals are calculated as mean +/- Z*SE\n",
    "    ci_minus = beta_mean - 1.645*var_beta\n",
    "    ci_plus = beta_mean + 1.645*var_beta\n",
    "\n",
    "    return ci_minus, ci_plus\n",
    "\n",
    "def plotFrankes(x_, y_, z_):\n",
    "    \"\"\"\n",
    "    Plot Franke's function\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    surf = ax.plot_surface(x_, y_, z_, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(-0.10, 1.40)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z - Franke')\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    clb = fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    clb.ax.set_title('Level')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#Ordinary Least Squared function\n",
    "def ols(x, y, z, degree = 5):\n",
    "    #x: vector of size(n, 1)\n",
    "    #y: vector of size(n,1)\n",
    "    #z: vector of size(n,1)\n",
    "    xyb_ = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    xyb = poly.fit_transform(xyb_)\n",
    "    beta = np.linalg.inv(xyb.T.dot(xyb)).dot(xyb.T).dot(z)\n",
    "\n",
    "    return beta\n",
    "\n",
    "def RidgeRegression(x, y, z, degree=5, l=0.0001):\n",
    "    \"\"\"\n",
    "    :param x: numpy vector of size (n, 1)\n",
    "    :param y: numpy vector of size (n, 1)\n",
    "    :param degree: degree of polynomial fit\n",
    "    :param l: Ridge penalty coefficient\n",
    "    :return: numpy array with the beta coefficients\n",
    "    \"\"\"\n",
    "    # Calculate matrix with x, y - polynomials\n",
    "    M_ = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    M = poly.fit_transform(M_)\n",
    "\n",
    "    # Calculate beta\n",
    "    A = np.arange(1, degree + 2)\n",
    "    rows = np.sum(A)\n",
    "    beta = (np.linalg.inv(M.T.dot(M) + l * np.identity(rows))).dot(M.T).dot(z)\n",
    "\n",
    "    return beta\n",
    "\n",
    "def Lasso(x, y, z, degree=5, a=1e-06):\n",
    "\n",
    "    X = np.c_[x, y]\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_ = poly.fit_transform(X)\n",
    "\n",
    "    clf = linear_model.Lasso(alpha=a, max_iter=5000, fit_intercept=False)\n",
    "    clf.fit(X_, z)\n",
    "    beta = clf.coef_\n",
    "\n",
    "    return beta\n",
    "\n",
    "def bootstrap(x, y, z, p_degree, method, n_bootstrap=100):\n",
    "    # Randomly shuffle data\n",
    "    data_set = np.c_[x, y, z]\n",
    "    np.random.shuffle(data_set)\n",
    "    set_size = round(len(x)/5)\n",
    "\n",
    "    # Extract test-set, never used in training. About 1/5 of total data\n",
    "    x_test = data_set[0:set_size, 0]\n",
    "    y_test = data_set[0:set_size, 1]\n",
    "    z_test = data_set[0:set_size, 2]\n",
    "    test_indices = np.linspace(0, set_size-1, set_size)\n",
    "\n",
    "    # And define the training set as the rest of the data\n",
    "    x_train = np.delete(data_set[:, 0], test_indices)\n",
    "    y_train = np.delete(data_set[:, 1], test_indices)\n",
    "    z_train = np.delete(data_set[:, 2], test_indices)\n",
    "\n",
    "    Z_predict = []\n",
    "\n",
    "    MSE = []\n",
    "    R2s = []\n",
    "    for i in range(n_bootstrap):\n",
    "        x_, y_, z_ = resample(x_train, y_train, z_train)\n",
    "\n",
    "        if method == 'Ridge':\n",
    "            # Ridge regression, save beta values\n",
    "            beta = RidgeRegression(x_, y_, z_, degree=p_degree)\n",
    "        elif method == 'Lasso':\n",
    "            beta = Lasso(x_, y_, z_, degree=p_degree)\n",
    "        elif method == 'OLS':\n",
    "            beta = ols(x_, y_, z_, degree=p_degree)\n",
    "        else:\n",
    "            print('ERROR: Cannot recognize method')\n",
    "            return 0\n",
    "\n",
    "        M_ = np.c_[x_test, y_test]\n",
    "        poly = PolynomialFeatures(p_degree)\n",
    "        M = poly.fit_transform(M_)\n",
    "        z_hat = M.dot(beta)\n",
    "\n",
    "        Z_predict.append(z_hat)\n",
    "\n",
    "        # Calculate MSE\n",
    "        MSE.append(np.mean((z_test - z_hat)**2))\n",
    "        R2s.append(R2(z_test, z_hat))\n",
    "\n",
    "    # Calculate MSE, Bias and Variance\n",
    "    MSE_M = np.mean(MSE)\n",
    "    R2_M = np.mean(R2s)\n",
    "    bias = np.mean((z_test - np.mean(Z_predict, axis=0, keepdims=True))**2)\n",
    "    variance = np.mean(np.var(Z_predict, axis=0, keepdims=True))\n",
    "    return MSE_M, R2_M, bias, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bod\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "C:\\Users\\bod\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:167: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "C:\\Users\\bod\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:168: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from sklearn.utils import resample\n",
    "\"\"\"\n",
    "    Analysis of bootstrap resampling technique\n",
    "\"\"\"\n",
    "# load data\n",
    "X = np.load('data.npy')\n",
    "x = X[:, 0]\n",
    "y = X[:, 1]\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "MSE, R2_b, bias, variance = bootstrap(x, y, z, method='OLS', p_degree=5)\n",
    "text_file = open(\"../Results/ex2/Bootstrap_ols.txt\", \"w\")\n",
    "text_file.write('--- BOOTSTRAP for OLS --- \\n')\n",
    "text_file.write('MSE: {} \\n'.format(MSE))\n",
    "text_file.write('R2: {} \\n'.format(R2_b))\n",
    "text_file.write('Bias: {} \\n'.format(bias))\n",
    "text_file.write('Variance: {} \\n'.format(variance))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
